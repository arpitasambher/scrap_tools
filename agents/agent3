
import json
from pathlib import Path
from openai import OpenAI
import subprocess

# Load the JSON data
with open("output27.json", "r", encoding="utf-8") as f:
    data = json.load(f)

summaries = []
for entry in data:
    url = entry.get("url", "No URL")
    scraped = entry.get("scraped_info", {})

    if isinstance(scraped, dict):
        # Handle scraping error
        if "error" in scraped:
            summaries.append(f"\n\n🔗 URL: {url}\n⚠️ Error: {scraped['error']}")
            continue

        summary_text = ""

        # Case 1: Flat 'text' or 'content' as string
        for key in ["text", "content", "description", "value"]:
            if isinstance(scraped.get(key), str) and scraped[key].strip():
                summary_text += scraped[key].strip() + "\n"

        # Case 2: Nested dictionary structures
        for key, val in scraped.items():
            if isinstance(val, dict):
                for subkey, subval in val.items():
                    if isinstance(subval, list):
                        for item in subval:
                            if isinstance(item, dict):
                                for v in item.values():
                                    if isinstance(v, str):
                                        summary_text += v.strip() + "\n"
                            elif isinstance(item, str):
                                summary_text += item.strip() + "\n"

            elif isinstance(val, list):
                for item in val:
                    if isinstance(item, dict):
                        for v in item.values():
                            if isinstance(v, str):
                                summary_text += v.strip() + "\n"
                    elif isinstance(item, str):
                        summary_text += item.strip() + "\n"

        # Append final formatted entry
        if summary_text.strip():
            summaries.append(f"\n\n🔗 URL: {url}\n{summary_text.strip()}")
        else:
            summaries.append(f"\n\n🔗 URL: {url}\n⚠️ No useful content extracted.")
    else:
        summaries.append(f"\n\n🔗 URL: {url}\n⚠️ Unrecognized content format.")

# Write combined input for LLM
summary_input_path = Path("agent3/summary_input.txt")
summary_input_path.write_text("\n".join(summaries), encoding="utf-8")

print("✅ Formatted summary input saved to agent3/summary_input.txt")

# 🔁 Ask Ollama to summarize
print("\n🧠 Generating summary using Ollama (llama3.2:1b)...\n")

ollama_prompt = f"""You are an investigative summarizer. Given the following text chunks and article extractions from multiple URLs, write a clear and concise summary paragraph **for each URL**. Focus only on content related to fraud, anti-money laundering (AML), or violence. Do not include links or unrelated text. Mention the **URL** followed by a **concise summary paragraph**.

Here is the combined extracted content:

{summary_input_path.read_text(encoding="utf-8")}
"""

# Run Ollama CLI
completed = subprocess.run(
    ["ollama", "run", "llama3.2:1b"],
    input=ollama_prompt.encode("utf-8"),
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE
)

# Handle result
output_text = completed.stdout.decode("utf-8", errors="ignore")
Path("agent3/summary.txt").write_text(output_text.strip(), encoding="utf-8")

print("✅ Summary saved to agent3/summary.txt")
